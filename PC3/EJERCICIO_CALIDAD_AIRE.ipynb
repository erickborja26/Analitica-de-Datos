{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b452ff",
   "metadata": {},
   "source": [
    "# EVALUACIÓN DE MÉTODOS DE ENSEMBLE LEARNING (PARTE 1)\n",
    "**Integrante 1:** Familia de Paralelización (Bagging, Pasting y Random Forests) \n",
    "\n",
    "**Tema:** Predicción de Calidad de Aire (Target: PM 2.5)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Introducción y Configuración\n",
    "En esta sección se implementarán 10 modelos predictivos enfocados en estrategias de reducción de varianza. Según la literatura del Capítulo 7, los métodos de conjunto como **Bagging** y **Random Forests** permiten entrenar predictores de manera independiente y paralela, promediando sus resultados para mejorar la generalización frente a un solo modelo base.\n",
    "\n",
    "Se utilizará un dataset de calidad de aire (Senamhi) para predecir la concentración de material particulado fino (**PM 2.5**), una variable crítica para la salud pública."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a25bb",
   "metadata": {},
   "source": [
    "### Configuración y Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4671a6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. CARGA DE DATOS ---\n",
      "[INFO] Cargando desde archivo local: ../PC1/senamhi_detalle_limpio.csv\n",
      "Carga exitosa.\n",
      "Dimensiones iniciales: (2123, 9)\n",
      "Columnas: ['Estacion', 'Fecha', 'Hora', 'PM 2.5', 'PM 10', 'SO2', 'NO2', 'O3', 'CO']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Configuración de estilo de gráficos\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"--- 1. CARGA DE DATOS ---\")\n",
    "\n",
    "# Definición de rutas (Estructura: Raíz -> PC1 (csv) | PC3 (notebook))\n",
    "ruta_local = '../PC1/senamhi_detalle_limpio.csv'\n",
    "url_raw = 'https://github.com/erickborja26/Analitica-de-Datos/raw/refs/heads/main/PC1/senamhi_detalle_limpio.csv'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(ruta_local):\n",
    "        print(f\"[INFO] Cargando desde archivo local: {ruta_local}\")\n",
    "        df = pd.read_csv(ruta_local)\n",
    "    else:\n",
    "        print(f\"[ALERTA] No se encontró {ruta_local}. Descargando desde GitHub...\")\n",
    "        df = pd.read_csv(url_raw)\n",
    "    \n",
    "    print(\"Carga exitosa.\")\n",
    "    print(f\"Dimensiones iniciales: {df.shape}\")\n",
    "    print(\"Columnas:\", df.columns.tolist())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] No se pudo cargar la data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6fbeff",
   "metadata": {},
   "source": [
    "### 2. Preprocesamiento (Adaptado al Dataset de SENAMHI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a91a88d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Experimental Setup: Multi-Output Regression\n",
      "Inputs (X): ['Hora_Num', 'Mes', 'Dia_Semana', 'Estacion_Code']\n",
      "Targets (Y): ['PM 2.5', 'PM 10', 'SO2', 'NO2', 'O3', 'CO']\n",
      "Muestras de Entrenamiento: 964\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURACIÓN DE DATOS MULTI-SALIDA ---\n",
    "\n",
    "# 1. Definimos el objetivo integral: Predecir todos los contaminantes\n",
    "# Esto permite evaluar qué modelo captura mejor la química atmosférica completa.\n",
    "TARGETS = ['PM 2.5', 'PM 10', 'SO2', 'NO2', 'O3', 'CO']\n",
    "\n",
    "# 2. Ingeniería de Características (Variables Predictoras)\n",
    "def procesar_hora(str_hora):\n",
    "    try:\n",
    "        return int(str_hora.split(':')[0])\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Generamos variables temporales\n",
    "if 'Hora' in df.columns:\n",
    "    df['Hora_Num'] = df['Hora'].apply(procesar_hora)\n",
    "\n",
    "if 'Fecha' in df.columns:\n",
    "    df['Fecha_Dt'] = pd.to_datetime(df['Fecha'], format='%d/%m/%Y', errors='coerce')\n",
    "    df['Mes'] = df['Fecha_Dt'].dt.month\n",
    "    df['Dia_Semana'] = df['Fecha_Dt'].dt.dayofweek\n",
    "\n",
    "# Codificación de Estación (si existe)\n",
    "if 'Estacion' in df.columns:\n",
    "    df['Estacion_Code'] = pd.factorize(df['Estacion'])[0]\n",
    "\n",
    "# Variables de entrada (X)\n",
    "features = ['Hora_Num', 'Mes', 'Dia_Semana', 'Estacion_Code']\n",
    "\n",
    "# 3. Limpieza estricta\n",
    "df_clean = df.dropna(subset=TARGETS + features)\n",
    "\n",
    "X = df_clean[features]\n",
    "y = df_clean[TARGETS]\n",
    "\n",
    "# 4. División de Datos\n",
    "# Usamos random_state=42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Experimental Setup: Multi-Output Regression\")\n",
    "print(f\"Inputs (X): {features}\")\n",
    "print(f\"Targets (Y): {TARGETS}\")\n",
    "print(f\"Muestras de Entrenamiento: {X_train.shape[0]}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "598c9b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listas de métricas inicializadas. Listo para entrenar bloques.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# Listas globales para acumular los resultados de todos los bloques\n",
    "metricas_globales = []\n",
    "print(\"Listas de métricas inicializadas. Listo para entrenar bloques.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a0f8b",
   "metadata": {},
   "source": [
    "### 3. Implementación: Línea Base y Familia Bagging\n",
    "A continuación, se entrenan los primeros 4 modelos.\n",
    "* **Modelo 1 (Línea Base):** Un Árbol de Decisión simple para tener una referencia mínima.\n",
    "* **Bagging:** Se utiliza `bootstrap=True` (muestreo con reemplazo). Esto permite que algunas instancias se repitan, aumentando ligeramente el sesgo pero reduciendo la varianza.\n",
    "* **Pasting:** Se utiliza `bootstrap=False` (muestreo sin reemplazo).\n",
    "* **Bagging Heterogéneo:** Se aplica la técnica de Bagging sobre un estimador diferente (SVR) para demostrar la flexibilidad del algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "612f9af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LÍNEA BASE Y BAGGING ---\n",
      "Entrenando Modelo 1: Decision Tree...\n",
      "Entrenando Modelo 2: Bagging (Con Reemplazo)...\n",
      "Entrenando Modelo 3: Pasting (Sin Reemplazo)...\n",
      "Entrenando Modelo 4: Bagging Heterogéneo...\n"
     ]
    }
   ],
   "source": [
    "print(\"--- LÍNEA BASE Y BAGGING ---\")\n",
    "\n",
    "# 1. Decision Tree (Línea Base)\n",
    "# Usamos un solo árbol para tener un punto de comparación mínimo.\n",
    "print(\"Entrenando Modelo 1: Decision Tree...\")\n",
    "m1 = DecisionTreeRegressor(random_state=42)\n",
    "start = time.time()\n",
    "m1.fit(X_train, y_train)\n",
    "t1 = time.time() - start\n",
    "metricas_globales.append({'Modelo': '1. Decision Tree', 'R2': m1.score(X_test, y_test), 'Tiempo': t1, 'Objeto': m1})\n",
    "\n",
    "# 2. Bagging Tradicional (Bootstrap=True)\n",
    "# El muestreo con reemplazo aumenta ligeramente el sesgo pero reduce la varianza.\n",
    "print(\"Entrenando Modelo 2: Bagging (Con Reemplazo)...\")\n",
    "m2 = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=50,\n",
    "    bootstrap=True,  # Bagging\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "start = time.time()\n",
    "m2.fit(X_train, y_train)\n",
    "t2 = time.time() - start\n",
    "metricas_globales.append({'Modelo': '2. Bagging Tree', 'R2': m2.score(X_test, y_test), 'Tiempo': t2, 'Objeto': m2})\n",
    "\n",
    "# 3. Pasting (Bootstrap=False)\n",
    "# Muestreo sin reemplazo. Útil en datasets muy grandes.\n",
    "print(\"Entrenando Modelo 3: Pasting (Sin Reemplazo)...\")\n",
    "m3 = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=50,\n",
    "    bootstrap=False, # Pasting\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "start = time.time()\n",
    "m3.fit(X_train, y_train)\n",
    "t3 = time.time() - start\n",
    "metricas_globales.append({'Modelo': '3. Pasting Tree', 'R2': m3.score(X_test, y_test), 'Tiempo': t3, 'Objeto': m3})\n",
    "\n",
    "# 4. Bagging Heterogéneo (SVR/KNN)\n",
    "print(\"Entrenando Modelo 4: Bagging Heterogéneo...\")\n",
    "m4 = BaggingRegressor(\n",
    "    estimator=KNeighborsRegressor(n_neighbors=5),\n",
    "    n_estimators=10,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "start = time.time()\n",
    "m4.fit(X_train, y_train)\n",
    "t4 = time.time() - start\n",
    "metricas_globales.append({'Modelo': '4. Bagging KNN', 'R2': m4.score(X_test, y_test), 'Tiempo': t4, 'Objeto': m4})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e79f2",
   "metadata": {},
   "source": [
    "### 4. Implementación: Random Forests\n",
    "El algoritmo **Random Forest** introduce aleatoriedad adicional al buscar la mejor característica dentro de un subconjunto aleatorio, en lugar de todas las disponibles.\n",
    "Se probarán tres configuraciones:\n",
    "* **Estándar:** Parámetros por defecto.\n",
    "* **Profundo:** Aumentando el número de árboles a 300 para capturar patrones más complejos.\n",
    "* **Regularizado:** Limitando la profundidad del árbol (`max_depth=10`) para controlar el sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba1be2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RANDOM FORESTS ---\n",
      "Entrenando Modelo 5: Random Forest Estándar...\n",
      "Entrenando Modelo 6: Random Forest Profundo (300 árboles)...\n",
      "Entrenando Modelo 7: Random Forest Regularizado...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- RANDOM FORESTS ---\")\n",
    "\n",
    "# 5. Random Forest Estándar\n",
    "# Configuración por defecto (100 árboles).\n",
    "print(\"Entrenando Modelo 5: Random Forest Estándar...\")\n",
    "m5 = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "start = time.time()\n",
    "m5.fit(X_train, y_train)\n",
    "t5 = time.time() - start\n",
    "metricas_globales.append({'Modelo': '5. RF Estándar', 'R2': m5.score(X_test, y_test), 'Tiempo': t5, 'Objeto': m5})\n",
    "\n",
    "# 6. Random Forest Profundo\n",
    "# Aumentamos el número de estimadores para capturar patrones más complejos.\n",
    "print(\"Entrenando Modelo 6: Random Forest Profundo (300 árboles)...\")\n",
    "m6 = RandomForestRegressor(n_estimators=300, max_depth=None, n_jobs=-1, random_state=42)\n",
    "start = time.time()\n",
    "m6.fit(X_train, y_train)\n",
    "t6 = time.time() - start\n",
    "metricas_globales.append({'Modelo': '6. RF Profundo', 'R2': m6.score(X_test, y_test), 'Tiempo': t6, 'Objeto': m6})\n",
    "\n",
    "# 7. Random Forest Regularizado\n",
    "# Limitamos la profundidad (max_depth) para combatir el overfitting.\n",
    "print(\"Entrenando Modelo 7: Random Forest Regularizado...\")\n",
    "m7 = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_leaf=5, n_jobs=-1, random_state=42)\n",
    "start = time.time()\n",
    "m7.fit(X_train, y_train)\n",
    "t7 = time.time() - start\n",
    "metricas_globales.append({'Modelo': '7. RF Regularizado', 'R2': m7.score(X_test, y_test), 'Tiempo': t7, 'Objeto': m7})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c4498",
   "metadata": {},
   "source": [
    "### 5. Implementación: Extra-Trees y Selección de Características\n",
    "Los **Extra-Trees** (Extremely Randomized Trees) utilizan umbrales aleatorios para cada característica, aumentando la velocidad de entrenamiento y reduciendo la varianza.\n",
    "Además, aprovechamos la capacidad de los bosques para medir la **importancia de las características** y entrenamos un último modelo optimizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bff255e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EXTRA-TREES Y FEATURES ---\n",
      "Entrenando Modelo 8: Extra-Trees Estándar...\n",
      "Entrenando Modelo 9: Extra-Trees Optimizado...\n",
      "Entrenando Modelo 10: RF con Selección de Características (Log2)...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- EXTRA-TREES Y FEATURES ---\")\n",
    "\n",
    "# 8. Extra-Trees Estándar\n",
    "# Utiliza umbrales aleatorios para cada característica, lo que lo hace más rápido.\n",
    "print(\"Entrenando Modelo 8: Extra-Trees Estándar...\")\n",
    "m8 = ExtraTreesRegressor(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "start = time.time()\n",
    "m8.fit(X_train, y_train)\n",
    "t8 = time.time() - start\n",
    "metricas_globales.append({'Modelo': '8. Extra-Trees', 'R2': m8.score(X_test, y_test), 'Tiempo': t8, 'Objeto': m8})\n",
    "\n",
    "# 9. Extra-Trees Optimizado\n",
    "# Ajustamos min_samples_split para suavizar el ruido de los sensores.\n",
    "print(\"Entrenando Modelo 9: Extra-Trees Optimizado...\")\n",
    "m9 = ExtraTreesRegressor(n_estimators=200, min_samples_split=10, n_jobs=-1, random_state=42)\n",
    "start = time.time()\n",
    "m9.fit(X_train, y_train)\n",
    "t9 = time.time() - start\n",
    "metricas_globales.append({'Modelo': '9. ET Optimizado', 'R2': m9.score(X_test, y_test), 'Tiempo': t9, 'Objeto': m9})\n",
    "\n",
    "# 10. Random Forest con Selección de Características (Log2)\n",
    "# Forzamos al modelo a ver menos características por división (max_features='log2').\n",
    "# Esto aumenta la diversidad de los árboles.\n",
    "print(\"Entrenando Modelo 10: RF con Selección de Características (Log2)...\")\n",
    "m10 = RandomForestRegressor(n_estimators=100, max_features='log2', n_jobs=-1, random_state=42)\n",
    "start = time.time()\n",
    "m10.fit(X_train, y_train)\n",
    "t10 = time.time() - start\n",
    "metricas_globales.append({'Modelo': '10. RF Log2 Feat', 'R2': m10.score(X_test, y_test), 'Tiempo': t10, 'Objeto': m10})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d7e63",
   "metadata": {},
   "source": [
    "### 5. Visualización de Resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e45388e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TABLA DE POSICIONES (INTEGRANTE 1) ---\n",
      "                Modelo        R2    Tiempo\n",
      "1     9. ET Optimizado  0.565693  0.126853\n",
      "2   7. RF Regularizado  0.519198  0.144613\n",
      "3       4. Bagging KNN  0.501632  0.072654\n",
      "4     10. RF Log2 Feat  0.444947  0.104420\n",
      "5      2. Bagging Tree  0.432354  5.239780\n",
      "6       6. RF Profundo  0.429598  0.311061\n",
      "7       5. RF Estándar  0.428532  0.119477\n",
      "8       8. Extra-Trees  0.223481  0.133094\n",
      "9      3. Pasting Tree  0.084758  0.184368\n",
      "10    1. Decision Tree  0.084042  0.007592\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Convertimos la lista acumulada en un DataFrame\n",
    "df_resultados = pd.DataFrame(metricas_globales)\n",
    "df_resultados = df_resultados.sort_values(by='R2', ascending=False)\n",
    "df_resultados = df_resultados.drop_duplicates(subset='Modelo', keep='last')\n",
    "df_resultados = df_resultados.reset_index(drop=True)\n",
    "df_resultados.index = df_resultados.index + 1\n",
    "\n",
    "print(\"\\n--- TABLA DE POSICIONES (INTEGRANTE 1) ---\")\n",
    "print(df_resultados[['Modelo', 'R2', 'Tiempo']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
